{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary extensions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "\n",
    "# Standard library imports\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party library imports\n",
    "import ipytest\n",
    "import json\n",
    "\n",
    "# Local imports\n",
    "from src.paths import CODE_FOLDER, DATA_DIR, INFERENCE_CODE_FOLDER, PARENT_DIR\n",
    "\n",
    "sys.path.append(str(PARENT_DIR / CODE_FOLDER))\n",
    "sys.path.append(str(PARENT_DIR / INFERENCE_CODE_FOLDER))\n",
    "\n",
    "DATA_FILE_PATH = DATA_DIR / \"penguins.csv\"\n",
    "\n",
    "ipytest.autoconfig(raise_on_error=True)\n",
    "\n",
    "# Prevent SageMaker SDK to log events related to the default\n",
    "# configuration using the INFO level\n",
    "logging.getLogger(\"sagemaker.config\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = os.getenv(\"BUCKET\")\n",
    "role = os.getenv(\"ROLE\")\n",
    "\n",
    "S3_LOCATION = f\"s3://{bucket}/penguins\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession, LocalPipelineSession\n",
    "\n",
    "pipeline_session = PipelineSession(default_bucket=bucket) if not LOCAL_MODE else None\n",
    "\n",
    "if LOCAL_MODE:\n",
    "    config = {\n",
    "        \"session\": LocalPipelineSession(default_bucket=bucket),\n",
    "        \"instance_type\": \"local\",\n",
    "        \"image\": None\n",
    "    }\n",
    "else:\n",
    "    config = {\n",
    "        \"session\": pipeline_session,\n",
    "        \"instance_type\": \"ml.m5.xlarge\",\n",
    "        \"image\": None\n",
    "    }\n",
    "\n",
    "config[\"framework_version\"] = \"2.11\"\n",
    "config[\"py_version\"] = \"py39\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "iam_client = boto3.client(\"iam\")\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of 'drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Preprocessing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /Users/carlos/Projects/penguin-classifier/src/preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/preprocessor.py\n",
    "\"\"\"\n",
    "This module preprocesses data for machine learning tasks. It includes functions to read data from CSV files,\n",
    "split the data into training, validation, and test sets, save baseline data, transform data, and save the\n",
    "processed data and models.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Import statements...\n",
    "from typing import Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def preprocess(base_directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Preprocess the data by loading, splitting, transforming, saving the splits, and saving the model.\n",
    "\n",
    "    Args:\n",
    "        base_directory: The base directory where the input data and outputs will be managed.\n",
    "    \"\"\"\n",
    "    # 1. Load supplied data\n",
    "    df = _read_data_from_csv_files(base_directory)\n",
    "\n",
    "    # 2. Split data into train and test sets\n",
    "    df_train, df_validation, df_test = _split_data(df)\n",
    "\n",
    "    # 3. Save baseline data\n",
    "    _save_baselines(base_directory, df_train, df_test)\n",
    "\n",
    "    # 3. Transform the train and test sets\n",
    "    target_transformer = ColumnTransformer(\n",
    "        transformers=[(\"species\", OrdinalEncoder(), [0])]\n",
    "    )\n",
    "\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"mean\"), StandardScaler()\n",
    "    )\n",
    "\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"), OneHotEncoder()\n",
    "    )\n",
    "\n",
    "    features_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"numeric\",\n",
    "                numeric_transformer,\n",
    "                make_column_selector(dtype_exclude=\"object\"),\n",
    "            ),\n",
    "            (\"categorical\", categorical_transformer, [\"island\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    y_train = target_transformer.fit_transform(\n",
    "        np.array(df_train.species.values).reshape(-1, 1)\n",
    "    )\n",
    "    y_validation = target_transformer.transform(\n",
    "        np.array(df_validation.species.values).reshape(-1, 1)\n",
    "    )\n",
    "    y_test = target_transformer.transform(\n",
    "        np.array(df_test.species.values).reshape(-1, 1)\n",
    "    )\n",
    "\n",
    "    df_train = df_train.drop(columns=[\"species\"], axis=1)\n",
    "    df_validation = df_validation.drop(columns=[\"species\"], axis=1)\n",
    "    df_test = df_test.drop(columns=[\"species\"], axis=1)\n",
    "\n",
    "    X_train = features_transformer.fit_transform(df_train)\n",
    "    X_validation = features_transformer.transform(df_validation)\n",
    "    X_test = features_transformer.transform(df_test)\n",
    "\n",
    "    # 4. Save the train and test splits\n",
    "    _save_splits(\n",
    "        base_directory, X_train, y_train, X_validation, y_validation, X_test, y_test\n",
    "    )\n",
    "\n",
    "    # 5. Save the model (transformers) in tar.gz format\n",
    "    _save_model(base_directory, target_transformer, features_transformer)\n",
    "\n",
    "\n",
    "def _read_data_from_csv_files(base_directory: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read and concatenate data from CSV files located in the input directory.\n",
    "\n",
    "    Args:\n",
    "        base_directory: The directory where CSV files are located.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the concatenated data.\n",
    "    \"\"\"\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = [file for file in input_directory.glob(\"*.csv\")]\n",
    "\n",
    "    if len(files) == 0:\n",
    "        raise ValueError(f\"No csv files found in {input_directory}\")\n",
    "\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "    df = pd.concat(raw_data)\n",
    "\n",
    "    # Shuffle the data\n",
    "    return df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "def _split_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split the DataFrame into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to be split.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the training, validation, and test DataFrames.\n",
    "    \"\"\"\n",
    "    df_train, temp = train_test_split(df, test_size=0.3)\n",
    "    df_validation, df_test = train_test_split(temp, test_size=0.5)\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "\n",
    "def _save_baselines(\n",
    "    base_directory: str, df_train: pd.DataFrame = None, df_test: pd.DataFrame = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save baseline versions of the training and test data sets.\n",
    "\n",
    "    Args:\n",
    "        base_directory: Directory where the baseline data will be saved.\n",
    "        df_train: Training data DataFrame.\n",
    "        df_test: Test data DataFrame.\n",
    "    \"\"\"\n",
    "    for split, data in [(\"train\", df_train), (\"test\", df_test)]:\n",
    "        baseline_path = Path(base_directory) / f\"{split}-baseline\"\n",
    "        baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df = data.copy().dropna()\n",
    "\n",
    "        # Save header only for the train baseline\n",
    "        header = True if split == \"train\" else False\n",
    "        df.to_csv(baseline_path / f\"{split}-baseline.csv\", index=False, header=header)\n",
    "\n",
    "\n",
    "def _save_splits(\n",
    "    base_directory: str,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_validation: np.ndarray,\n",
    "    y_validation: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save the training, validation, and test sets after concatenating features with their respective targets.\n",
    "\n",
    "    Args:\n",
    "        base_directory: Directory where the data splits will be saved.\n",
    "        X_train: Features of the training set.\n",
    "        y_train: Target of the training set.\n",
    "        X_validation: Features of the validation set.\n",
    "        y_validation: Target of the validation set.\n",
    "        X_test: Features of the test set.\n",
    "        y_test: Target of the test set.\n",
    "    \"\"\"\n",
    "    train = np.concatenate((X_train, y_train), axis=1)\n",
    "    validation = np.concatenate((X_validation, y_validation), axis=1)\n",
    "    test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    validation_path = Path(base_directory) / \"validation\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", index=False, header=False)\n",
    "    pd.DataFrame(validation).to_csv(\n",
    "        validation_path / \"validation.csv\", index=False, header=False\n",
    "    )\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", index=False, header=False)\n",
    "\n",
    "\n",
    "def _save_model(base_directory: str, target_transformer, features_transformers) -> None:\n",
    "    \"\"\"\n",
    "    Save the preprocessing model (transformers) in tar.gz format.\n",
    "\n",
    "    Args:\n",
    "        base_directory: Directory where the model will be saved.\n",
    "        target_transformer: The transformer used for the target variable.\n",
    "        features_transformers: The transformers used for the feature variables.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as directory:\n",
    "        joblib.dump(target_transformer, os.path.join(directory, \"target.joblib\"))\n",
    "        joblib.dump(features_transformers, os.path.join(directory, \"features.joblib\"))\n",
    "\n",
    "        model_path = Path(base_directory) / \"model\"\n",
    "        model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with tarfile.open(f\"{str(model_path / 'model.tar.gz')}\", \"w:gz\") as tar:\n",
    "            tar.add(os.path.join(directory, \"target.joblib\"), arcname=\"target.joblib\")\n",
    "            tar.add(\n",
    "                os.path.join(directory, \"features.joblib\"), arcname=\"features.joblib\"\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(base_directory=\"/opt/ml/processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.16s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import tarfile\n",
    "from preprocessor import preprocess\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"function\", autouse=False)\n",
    "def directory():\n",
    "    directory = tempfile.mkdtemp()\n",
    "    \n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    shutil.copy2(DATA_FILE_PATH, input_directory / \"data.csv\")\n",
    "\n",
    "    directory = Path(directory)\n",
    "    preprocess(directory)\n",
    "\n",
    "    yield directory\n",
    "    \n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "def test_preprocess_generate_baselines(directory):\n",
    "    output_directories = os.listdir(directory)\n",
    "\n",
    "    assert \"train-baseline\" in output_directories\n",
    "    assert \"test-baseline\" in output_directories\n",
    "\n",
    "\n",
    "def test_preprocess_generate_data_splits(directory):\n",
    "    output_directories = os.listdir(directory)\n",
    "\n",
    "    assert \"train\" in output_directories\n",
    "    assert \"validation\" in output_directories\n",
    "    assert \"test\" in output_directories\n",
    "\n",
    "\n",
    "def test_preprocess_creates_two_models(directory):\n",
    "    model_path = directory / \"model\"\n",
    "\n",
    "    tar = tarfile.open(model_path / \"model.tar.gz\", \"r:gz\")\n",
    "\n",
    "    assert \"target.joblib\" in tar.getnames()\n",
    "    assert \"features.joblib\" in tar.getnames()\n",
    "\n",
    "\n",
    "def tests_splits_are_transformed(directory):\n",
    "    train = pd.read_csv(directory / \"train\" / \"train.csv\", header=None)\n",
    "    validation = pd.read_csv(directory / \"validation\" / \"validation.csv\", header=None)\n",
    "    test = pd.read_csv(directory / \"test\" / \"test.csv\", header=None)\n",
    "\n",
    "    # The number of features should be 7\n",
    "    # * 3 - island (one-hot encoded)\n",
    "    # * 1 - culmen_length_mm\n",
    "    # * 1 - culmen_depth_mm\n",
    "    # * 1 - flipper_length_mm\n",
    "    # * 1 - body_mass_g\n",
    "    num_features = 7\n",
    "\n",
    "    # The number of targets should be 1\n",
    "    assert train.shape[1] == num_features + 1\n",
    "    assert validation.shape[1] == num_features + 1\n",
    "    assert test.shape[1] == num_features + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Processing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"P15D\") # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString\n",
    "\n",
    "dataset_location = ParameterString(\n",
    "    name=\"dataset_location\",\n",
    "    default_value=f\"{S3_LOCATION}/data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    framework_version=\"1.2-1\",\n",
    "    base_job_name=\"preprocess-data\",\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlos/Projects/penguin-classifier/.venv/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"preprocess-data\",\n",
    "    step_args=processor.run(\n",
    "        code=f\"{CODE_FOLDER}/preprocessor.py\",\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=dataset_location,\n",
    "                destination=\"/opt/ml/processing/input\"\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train\",\n",
    "                source=\"/opt/ml/processing/train\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"validation\",\n",
    "                source=\"/opt/ml/processing/validation\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/validation\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test\",\n",
    "                source=\"/opt/ml/processing/test\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"model\",\n",
    "                source=\"/opt/ml/processing/model\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/model\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train-baseline\",\n",
    "                source=\"/opt/ml/processing/train-baseline\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train-baseline\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test-baseline\",\n",
    "                source=\"/opt/ml/processing/test-baseline\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test-baseline\",\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-2:035306718946:pipeline/preprocessing-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': '6409536e-96b4-4bdf-997d-b4670a8ada1a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '6409536e-96b4-4bdf-997d-b4670a8ada1a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '90',\n",
       "   'date': 'Fri, 01 Dec 2023 13:17:43 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "\n",
    "pipeline_definition_config = PipelineDefinitionConfig(\n",
    "    use_custom_job_prefix=True\n",
    ")\n",
    "\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    name=\"preprocessing-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[preprocessing_step],\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    ")\n",
    "\n",
    "preprocessing_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-2:035306718946:pipeline/preprocessing-pipeline/execution/5msi3g8gmcum', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x1032fcd50>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "preprocessing_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models and the Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /Users/carlos/Projects/penguin-classifier/src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "def train(model_directory: str, train_path: str, validation_path: str, epochs: int =50, batch_size: int=32) -> None:\n",
    "    \"\"\"\n",
    "    Train a model using the training and validation data sets.\n",
    "\n",
    "    Args:\n",
    "        model_directory: Directory where the model will be saved.\n",
    "        train_path: Path to the training data set.\n",
    "        validation_path: Path to the validation data set.\n",
    "        epochs: Number of epochs to train the model.\n",
    "        batch_size: Batch size used during training.\n",
    "    \"\"\"\n",
    "    # Load training and validation data sets\n",
    "    X_train = pd.read_csv(Path(train_path) / \"train.csv\")\n",
    "    y_train = X_train[X_train.columns[-1]]\n",
    "    X_train.drop(X_train.columns[-1], axis=1, inplace=True)\n",
    "\n",
    "    X_validation = pd.read_csv(Path(validation_path) / \"validation.csv\")\n",
    "    y_validation = X_validation[X_validation.columns[-1]]\n",
    "    X_validation.drop(X_validation.columns[-1], axis=1, inplace=True)\n",
    "\n",
    "    # Build a Sequential model\n",
    "    model = Sequential([\n",
    "        Dense(10, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "        Dense(8, activation=\"relu\"),\n",
    "        Dense(3, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=SGD(learning_rate=0.01),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_validation, y_validation),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_validation)\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    print(f\"Validation accuracy: {accuracy_score(y_validation, predictions)}\")\n",
    "\n",
    "    # Save the model\n",
    "    model_filepath = Path(model_directory) / \"001\"\n",
    "    model.save(model_filepath)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    train(\n",
    "        model_directory=os.environ[\"SM_MODEL_DIR\"],\n",
    "        train_path=os.environ[\"SM_CHANNEL_TRAIN\"],\n",
    "        validation_path=os.environ[\"SM_CHANNEL_VALIDATION\"],\n",
    "\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 10:17:46.774914: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 1s - loss: 1.2502 - accuracy: 0.1339 - val_loss: 1.1430 - val_accuracy: 0.1765 - 604ms/epoch - 76ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Validation accuracy: 0.17647058823529413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpvbo9zizm/model/001/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 1.41s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import pytest\n",
    "import tempfile\n",
    "import joblib\n",
    "\n",
    "from preprocessor import preprocess\n",
    "from train import train\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"function\", autouse=False)\n",
    "def directory():\n",
    "    directory = tempfile.mkdtemp()\n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(DATA_FILE_PATH, input_directory / \"data.csv\")\n",
    "    \n",
    "    directory = Path(directory)\n",
    "    \n",
    "    preprocess(base_directory=directory)\n",
    "    train(\n",
    "        model_directory=directory / \"model\",\n",
    "        train_path=directory / \"train\", \n",
    "        validation_path=directory / \"validation\",\n",
    "        epochs=1\n",
    "    )\n",
    "    \n",
    "    yield directory\n",
    "    \n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "def test_train_saves_a_folder_with_model_assets(directory):\n",
    "    output = os.listdir(directory / \"model\")\n",
    "    assert \"001\" in output\n",
    "    \n",
    "    assets = os.listdir(directory / \"model\" / \"001\")\n",
    "    assert \"saved_model.pb\" in assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    base_job_name=\"training\",\n",
    "    entry_point=f\"{CODE_FOLDER}/train.py\",\n",
    "    hyperparameters={\n",
    "        \"epochs\": 50,\n",
    "        \"batch_size\": 32\n",
    "    },\n",
    "    metrics_definitions=[\n",
    "        {\"Name\": \"loss\", \"Regex\": \"loss: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"accuracy\", \"Regex\": \"accuracy: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"val_loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}\n",
    "    ],\n",
    "    image_uri=config[\"image\"],\n",
    "    framework_version=config[\"framework_version\"],\n",
    "    py_version=config[\"py_version\"],\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    disable_profiler=True,\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlos/Projects/penguin-classifier/.venv/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_model_step = TrainingStep(\n",
    "    name=\"train-model\",\n",
    "    step_args=estimator.fit(\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"validation\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "        }\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TUNING_STEP = False and not LOCAL_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.parameter import IntegerParameter\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name=\"val_accuracy\",\n",
    "    objective_type=\"Maximize\",\n",
    "    hyperparameter_ranges={\n",
    "        \"epochs\": IntegerParameter(10, 50),\n",
    "    },\n",
    "    metric_definitions=[{\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}],\n",
    "    max_jobs=3,\n",
    "    max_parallel_jobs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import TuningStep\n",
    "\n",
    "tune_model_step = TuningStep(\n",
    "    name=\"tune-model\",\n",
    "    step_args=tuner.fit(\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"validation\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "        },\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-2:035306718946:pipeline/training-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': '28df84da-146e-4559-a031-ef71c4d8eee9',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '28df84da-146e-4559-a031-ef71c4d8eee9',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '85',\n",
       "   'date': 'Fri, 01 Dec 2023 13:17:51 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pipeline = Pipeline(\n",
    "    name=\"training-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "        tune_model_step if USE_TUNING_STEP else train_model_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")\n",
    "\n",
    "training_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-2:035306718946:pipeline/training-pipeline/execution/nsyiike64qs7', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x1032fcd50>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "training_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating and Versioning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /Users/carlos/Projects/penguin-classifier/src/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/evaluation.py\n",
    "\n",
    "import json\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def evaluate(model_path: str, test_path: str, output_path: str) -> None:\n",
    "    X_test = pd.read_csv(Path(test_path) / \"test.csv\")\n",
    "    y_test = X_test[X_test.columns[-1]]\n",
    "    X_test.drop(X_test.columns[-1], axis=1, inplace=True)\n",
    "\n",
    "    # Let's now extract the model package so we can load \n",
    "    # it in memory.\n",
    "    with tarfile.open(Path(model_path) / \"model.tar.gz\") as tar:\n",
    "        tar.extractall(path=Path(model_path))\n",
    "\n",
    "    model = keras.models.load_model(Path(model_path) / \"001\")\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "    evaluation_report = {\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": {\n",
    "                \"value\": accuracy\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    with open(Path(output_path) / \"evaluation.json\", \"w\") as f:\n",
    "        f.write(json.dumps(evaluation_report))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate(\n",
    "        model_path=\"/opt/ml/processing/model/\", \n",
    "        test_path=\"/opt/ml/processing/test/\",\n",
    "        output_path=\"/opt/ml/processing/evaluation/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 0s - loss: 1.1274 - accuracy: 0.2427 - val_loss: 1.1343 - val_accuracy: 0.2157 - 497ms/epoch - 62ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x13edc1d00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "Validation accuracy: 0.21568627450980393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmp4noe6nxa/model/001/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "Test accuracy: 0.11764705882352941\n",
      "\u001b[32m.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x14e4c2b60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 1s - loss: 1.4623 - accuracy: 0.1967 - val_loss: 1.3159 - val_accuracy: 0.2549 - 524ms/epoch - 65ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Validation accuracy: 0.2549019607843137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpo4x2rbho/model/001/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "Test accuracy: 0.1568627450980392\n",
      "\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 2.85s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "#| code-fold: true\n",
    "#| output: false\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import pytest\n",
    "import tempfile\n",
    "import joblib\n",
    "\n",
    "from preprocessor import preprocess\n",
    "from train import train\n",
    "from evaluation import evaluate\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"function\", autouse=False)\n",
    "def directory():\n",
    "    directory = tempfile.mkdtemp()\n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(DATA_FILE_PATH, input_directory / \"data.csv\")\n",
    "    \n",
    "    directory = Path(directory)\n",
    "    \n",
    "    preprocess(base_directory=directory)\n",
    "    \n",
    "    train(\n",
    "        model_directory=directory / \"model\",\n",
    "        train_path=directory / \"train\", \n",
    "        validation_path=directory / \"validation\",\n",
    "        epochs=1\n",
    "    )\n",
    "    \n",
    "    # After training a model, we need to prepare a package just like\n",
    "    # SageMaker would. This package is what the evaluation script is\n",
    "    # expecting as an input.\n",
    "    with tarfile.open(directory / \"model.tar.gz\", \"w:gz\") as tar:\n",
    "        tar.add(directory / \"model\" / \"001\", arcname=\"001\")\n",
    "        \n",
    "    evaluate(\n",
    "        model_path=directory, \n",
    "        test_path=directory / \"test\",\n",
    "        output_path=directory / \"evaluation\",\n",
    "    )\n",
    "\n",
    "    yield directory / \"evaluation\"\n",
    "    \n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "def test_evaluate_generates_evaluation_report(directory):\n",
    "    output = os.listdir(directory)\n",
    "    assert \"evaluation.json\" in output\n",
    "\n",
    "\n",
    "def test_evaluation_report_contains_accuracy(directory):\n",
    "    with open(directory / \"evaluation.json\", 'r') as file:\n",
    "        report = json.load(file)\n",
    "        \n",
    "    assert \"metrics\" in report\n",
    "    assert \"accuracy\" in report[\"metrics\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlowProcessor\n",
    "\n",
    "evaluation_processor = TensorFlowProcessor(\n",
    "    base_job_name=\"evaluation-processor\",\n",
    "    image_uri=config[\"image\"],\n",
    "    framework_version=config[\"framework_version\"],\n",
    "    py_version=config[\"py_version\"],\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_assets = train_model_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    "\n",
    "if USE_TUNING_STEP:\n",
    "    model_assets = tune_model_step.get_top_model_s3_uri(\n",
    "        top_k=0, s3_bucket=config[\"session\"].default_bucket()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"evaluation-report\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlos/Projects/penguin-classifier/.venv/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluate_model_step = ProcessingStep(\n",
    "    name=\"evaluate-model\",\n",
    "    step_args=evaluation_processor.run(\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"test\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/test\",\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=model_assets,\n",
    "                destination=\"/opt/ml/processing/model\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"\n",
    "            ),\n",
    "        ],\n",
    "        code=f\"{CODE_FOLDER}/evaluation.py\",\n",
    "    ),\n",
    "    property_files=[evaluation_report],\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PACKAGE_GROUP = \"penguins\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "tensorflow_model = TensorFlowModel(\n",
    "    model_data=model_assets,\n",
    "    framework_version=config[\"framework_version\"],\n",
    "    role=role,\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import ModelMetrics, MetricsSource\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=Join(\n",
    "            on=\"/\",\n",
    "            values=[\n",
    "                evaluate_model_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"evaluation\"\n",
    "                ].S3Output.S3Uri,\n",
    "                \"evaluation.json\",\n",
    "            ],\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.tensorflow.model:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "register_model_step = ModelStep(\n",
    "    name=\"register-model\",\n",
    "    step_args=tensorflow_model.register(\n",
    "        model_package_group_name=MODEL_PACKAGE_GROUP,\n",
    "        approval_status=\"Approved\",\n",
    "        model_metrics=model_metrics,\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"application/json\"],\n",
    "        inference_instances=[config[\"instance_type\"]],\n",
    "        transform_instances=[config[\"instance_type\"]],\n",
    "        domain=\"MACHINE_LEARNING\",\n",
    "        task=\"CLASSIFICATION\",\n",
    "        framework=\"TENSORFLOW\",\n",
    "        framework_version=config[\"framework_version\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Condition Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterFloat\n",
    "\n",
    "accuracy_threshold = ParameterFloat(name=\"accuracy_threshold\", default_value=0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.fail_step import FailStep\n",
    "\n",
    "fail_step = FailStep(\n",
    "    name=\"fail\",\n",
    "    error_message=Join(\n",
    "        on=\" \",\n",
    "        values=[\n",
    "            \"Execution failed because the model's accuracy was lower than\",\n",
    "            accuracy_threshold,\n",
    "        ],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "\n",
    "condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluate_model_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"metrics.accuracy.value\",\n",
    "    ),\n",
    "    right=accuracy_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "\n",
    "condition_step = ConditionStep(\n",
    "    name=\"check-model-accuracy\",\n",
    "    conditions=[condition],\n",
    "    if_steps=[register_model_step] if not LOCAL_MODE else [],\n",
    "    else_steps=[fail_step],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded None to s3://sigmoidal-bucket/evaluating-pipeline/code/968040ffa8955b343adf7f02b3f05a26/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sigmoidal-bucket/evaluating-pipeline/code/2c207c809cb0e0e9a1d77e5247f961f9/runproc.sh\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded None to s3://sigmoidal-bucket/evaluating-pipeline/code/968040ffa8955b343adf7f02b3f05a26/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sigmoidal-bucket/evaluating-pipeline/code/2c207c809cb0e0e9a1d77e5247f961f9/runproc.sh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-2:035306718946:pipeline/evaluating-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': 'a729b39f-5ac4-4022-811e-6e0e30d37783',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'a729b39f-5ac4-4022-811e-6e0e30d37783',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '87',\n",
       "   'date': 'Fri, 01 Dec 2023 13:51:08 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluating_pipeline = Pipeline(\n",
    "    name=\"evaluating-pipeline\",\n",
    "    parameters=[dataset_location, accuracy_threshold],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "        tune_model_step if USE_TUNING_STEP else train_model_step,\n",
    "        evaluate_model_step,\n",
    "        condition_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")\n",
    "\n",
    "evaluating_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-2:035306718946:pipeline/evaluating-pipeline/execution/lqfsi88fhfkb', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x1032fcd50>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluating_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Models and Serving Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "ENDPOINT = \"penguins-endpoint\"\n",
    "DATA_CAPTURE_DESTINATION = f\"{S3_LOCATION}/monitoring/data-capture\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelPackageGroupName': 'penguins',\n",
       " 'ModelPackageVersion': 2,\n",
       " 'ModelPackageArn': 'arn:aws:sagemaker:us-east-2:035306718946:model-package/penguins/2',\n",
       " 'CreationTime': datetime.datetime(2023, 12, 1, 11, 3, 56, 501000, tzinfo=tzlocal()),\n",
       " 'ModelPackageStatus': 'Completed',\n",
       " 'ModelApprovalStatus': 'Approved'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deploying model from registry\n",
    "response = sagemaker_client.list_model_packages(\n",
    "    ModelPackageGroupName=MODEL_PACKAGE_GROUP,\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    "    SortBy=\"CreationTime\",\n",
    "    MaxResults=1,\n",
    ")\n",
    "\n",
    "package = (\n",
    "    response[\"ModelPackageSummaryList\"][0]\n",
    "    if response[\"ModelPackageSummaryList\"]\n",
    "    else None\n",
    ")\n",
    "package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import ModelPackage\n",
    "\n",
    "if package:\n",
    "    model_package = ModelPackage(\n",
    "        model_package_arn=package[\"ModelPackageArn\"],\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: penguins-2023-12-01-14-05-01-090\n",
      "INFO:sagemaker:Creating endpoint-config with name penguins-endpoint\n",
      "INFO:sagemaker:Creating endpoint with name penguins-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---!"
     ]
    }
   ],
   "source": [
    "model_package.deploy(\n",
    "    endpoint_name=ENDPOINT, \n",
    "    initial_instance_count=1, \n",
    "    instance_type=config[\"instance_type\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = \"\"\"\n",
    "0.6569590202313976,-1.0813829646495108,1.2097102831892812,0.9226343641317372,1.0,0.0,0.0\n",
    "-0.7751048801481084,0.8822689351285553,-1.2168066120762704,0.9226343641317372,0.0,1.0,0.0\n",
    "-0.837387834894918,0.3386660813829646,-0.26237731892812,-1.92351941317372,0.0,0.0,1.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predictions\": [\n",
      "    [\n",
      "      0.0326238759,\n",
      "      0.00740541192,\n",
      "      0.959970713\n",
      "    ],\n",
      "    [\n",
      "      0.783189774,\n",
      "      0.148078725,\n",
      "      0.0687314644\n",
      "    ],\n",
      "    [\n",
      "      0.971578062,\n",
      "      0.0213339273,\n",
      "      0.00708800321\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\n",
      "Species: [2 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictor = Predictor(endpoint_name=ENDPOINT)\n",
    "\n",
    "try:\n",
    "    response = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n",
    "    response = json.loads(response.decode(\"utf-8\"))\n",
    "\n",
    "    print(json.dumps(response, indent=2))\n",
    "    print(f\"\\nSpecies: {np.argmax(response['predictions'], axis=1)}\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: penguins-endpoint\n",
      "INFO:sagemaker:Deleting endpoint with name: penguins-endpoint\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/carlos/Projects/penguin-classifier/src/inference/preprocessing_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {INFERENCE_CODE_FOLDER}/preprocessing_component.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "try:\n",
    "    from sagemaker_containers.beta.framework import encoders, worker\n",
    "except ImportError:\n",
    "    # We don't have access to the `worker` instance when testing locally.\n",
    "    # We'll set it to None so we can change the way functions create\n",
    "    # a response.\n",
    "    worker = None\n",
    "\n",
    "\n",
    "TARGET_COLUMN = \"species\"\n",
    "FEATURE_COLUMNS = [\n",
    "    \"island\",\n",
    "    \"culmen_length_mm\",\n",
    "    \"culmen_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "    \"body_mass_g\",\n",
    "    \"sex\",\n",
    "]\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Deserializes the model that will be used in this container.\n",
    "    \"\"\"\n",
    "\n",
    "    return joblib.load(os.path.join(model_dir, \"features.joblib\"))\n",
    "    \n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    \"\"\"\n",
    "    Parses the input payload and creates a Pandas DataFrame.\n",
    "\n",
    "    This function will check whether the target column is present in the\n",
    "    input data, and will remove it.\n",
    "    \"\"\"\n",
    "\n",
    "    if content_type == \"text/csv\":\n",
    "        df = pd.read_csv(StringIO(input_data), header=None, skipinitialspace=True)\n",
    "\n",
    "        if len(df.columns) == len(FEATURE_COLUMNS) + 1:\n",
    "            df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "        df.columns = FEATURE_COLUMNS\n",
    "        return df\n",
    "\n",
    "    if content_type == \"application/json\":\n",
    "        df = pd.DataFrame([json.loads(input_data)])\n",
    "\n",
    "        if \"species\" in df.columns:\n",
    "            df = df.drop(\"species\", axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    raise ValueError(f\"{content_type} is not supported!\")\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"\n",
    "    Preprocess the input using the transformer.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.transform(input_data)\n",
    "        return response\n",
    "    except ValueError as e:\n",
    "        print(\"Error transforming the input data\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"\n",
    "    Formats the prediction output to generate a response.\n",
    "\n",
    "    The default accept/content-type between containers for serial inference\n",
    "    is JSON. Since this model will preceed a TensorFlow model, we want to\n",
    "    return a JSON object following TensorFlow's input requirements.\n",
    "    \"\"\"\n",
    "\n",
    "    if prediction is None:\n",
    "        raise Exception(\"There was an error transforming the input data\")\n",
    "\n",
    "    instances = [p for p in prediction.tolist()]\n",
    "    response = {\"instances\": instances}\n",
    "    return (\n",
    "        worker.Response(json.dumps(response), mimetype=accept)\n",
    "        if worker\n",
    "        else (response, accept)\n",
    "    )\n",
    "\n",
    "    raise Exception(f\"{accept} accept type is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                     [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m8 passed\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "from preprocessing_component import input_fn, predict_fn, output_fn, model_fn\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"function\", autouse=False)\n",
    "def directory():\n",
    "    directory = tempfile.mkdtemp()\n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(DATA_FILE_PATH, input_directory / \"data.csv\")\n",
    "    \n",
    "    directory = Path(directory)\n",
    "    \n",
    "    preprocess(base_directory=directory)\n",
    "    \n",
    "    with tarfile.open(directory / \"model\" / \"model.tar.gz\") as tar:\n",
    "        tar.extractall(path=directory / \"model\")\n",
    "    \n",
    "    yield directory / \"model\"\n",
    "    \n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "\n",
    "def test_input_csv_drops_target_column_if_present():\n",
    "    input_data = \"\"\"\n",
    "    Adelie, Torgersen, 39.1, 18.7, 181, 3750, MALE\n",
    "    \"\"\"\n",
    "    \n",
    "    df = input_fn(input_data, \"text/csv\")\n",
    "    assert len(df.columns) == 6 and \"species\" not in df.columns\n",
    "\n",
    "\n",
    "def test_input_json_drops_target_column_if_present():\n",
    "    input_data = json.dumps({\n",
    "        \"species\": \"Adelie\", \n",
    "        \"island\": \"Torgersen\",\n",
    "        \"culmen_length_mm\": 44.1,\n",
    "        \"culmen_depth_mm\": 18.0,\n",
    "        \"flipper_length_mm\": 210.0,\n",
    "        \"body_mass_g\": 4000.0,\n",
    "        \"sex\": \"MALE\"\n",
    "    })\n",
    "    \n",
    "    df = input_fn(input_data, \"application/json\")\n",
    "    assert len(df.columns) == 6 and \"species\" not in df.columns\n",
    "\n",
    "\n",
    "def test_input_csv_works_without_target_column():\n",
    "    input_data = \"\"\"\n",
    "    Torgersen, 39.1, 18.7, 181, 3750, MALE\n",
    "    \"\"\"\n",
    "    \n",
    "    df = input_fn(input_data, \"text/csv\")\n",
    "    assert len(df.columns) == 6\n",
    "\n",
    "\n",
    "def test_input_json_works_without_target_column():\n",
    "    input_data = json.dumps({\n",
    "        \"island\": \"Torgersen\",\n",
    "        \"culmen_length_mm\": 44.1,\n",
    "        \"culmen_depth_mm\": 18.0,\n",
    "        \"flipper_length_mm\": 210.0,\n",
    "        \"body_mass_g\": 4000.0,\n",
    "        \"sex\": \"MALE\"\n",
    "    })\n",
    "    \n",
    "    df = input_fn(input_data, \"application/json\")\n",
    "    assert len(df.columns) == 6\n",
    "\n",
    "\n",
    "def test_output_raises_exception_if_prediction_is_none():\n",
    "    with pytest.raises(Exception):\n",
    "        output_fn(None, \"application/json\")\n",
    "    \n",
    "    \n",
    "def test_output_returns_tensorflow_ready_input():\n",
    "    prediction = np.array([\n",
    "        [-1.3944109908736013,1.15488062669371,-0.7954340636549508,-0.5536447804097907,0.0,1.0,0.0],\n",
    "        [1.0557485835338234,0.5040085971987002,-0.5824506029515057,-0.5851840035995248,0.0,1.0,0.0]\n",
    "    ])\n",
    "    \n",
    "    response = output_fn(prediction, \"application/json\")\n",
    "    \n",
    "    assert response[0] == {\n",
    "        \"instances\": [\n",
    "            [-1.3944109908736013,1.15488062669371,-0.7954340636549508,-0.5536447804097907,0.0,1.0,0.0],\n",
    "            [1.0557485835338234,0.5040085971987002,-0.5824506029515057,-0.5851840035995248,0.0,1.0,0.0]\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    assert response[1] == \"application/json\"\n",
    "\n",
    "    \n",
    "def test_predict_transforms_data(directory):\n",
    "    input_data = \"\"\"\n",
    "    Torgersen, 39.1, 18.7, 181, 3750, MALE\n",
    "    \"\"\"\n",
    "    \n",
    "    model = model_fn(str(directory))\n",
    "    df = input_fn(input_data, \"text/csv\")\n",
    "    response = predict_fn(df, model)\n",
    "    assert type(response) is np.ndarray\n",
    "    \n",
    "\n",
    "def test_predict_returns_none_if_invalid_input(directory):\n",
    "    input_data = \"\"\"\n",
    "    Invalid, 39.1, 18.7, 181, 3750, MALE\n",
    "    \"\"\"\n",
    "    \n",
    "    model = model_fn(str(directory))\n",
    "    df = input_fn(input_data, \"text/csv\")\n",
    "    assert predict_fn(df, model) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/carlos/Projects/penguin-classifier/src/inference/postprocessing_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {INFERENCE_CODE_FOLDER}/postprocessing_component.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "\n",
    "try:\n",
    "    from sagemaker_containers.beta.framework import encoders, worker\n",
    "except ImportError:\n",
    "    # We don't have access to the `worker` instance when testing locally.\n",
    "    # We'll set it to None so we can change the way functions create\n",
    "    # a response.\n",
    "    worker = None\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Deserializes the target model and returns the list of fitted categories.\n",
    "    \"\"\"\n",
    "\n",
    "    model = joblib.load(os.path.join(model_dir, \"target.joblib\"))\n",
    "    return model.named_transformers_[\"species\"].categories_[0]\n",
    "\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    if content_type == \"application/json\":\n",
    "        predictions = json.loads(input_data)[\"predictions\"]\n",
    "        return predictions\n",
    "    \n",
    "    raise ValueError(f\"{content_type} is not supported.!\")\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"\n",
    "    Transforms the prediction into its corresponding category.\n",
    "    \"\"\"\n",
    "    predictions = np.argmax(input_data, axis=-1)\n",
    "    confidence = np.max(input_data, axis=-1)\n",
    "    return [\n",
    "        (model[prediction], confidence)\n",
    "        for confidence, prediction in zip(confidence, predictions)\n",
    "    ]\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    if accept == \"text/csv\":\n",
    "        return (\n",
    "            worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n",
    "            if worker\n",
    "            else (prediction, accept)\n",
    "        )\n",
    "\n",
    "    if accept == \"application/json\":\n",
    "        response = []\n",
    "        for p, c in prediction:\n",
    "            response.append({\"prediction\": p, \"confidence\": c})\n",
    "\n",
    "        # If there's only one prediction, we'll return it\n",
    "        # as a single object.\n",
    "        if len(response) == 1:\n",
    "            response = response[0]\n",
    "\n",
    "        return (\n",
    "            worker.Response(json.dumps(response), mimetype=accept)\n",
    "            if worker\n",
    "            else (response, accept)\n",
    "        )\n",
    "\n",
    "    raise Exception(f\"{accept} accept type is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from postprocessing_component import predict_fn, output_fn\n",
    "\n",
    "\n",
    "def test_predict_returns_prediction_as_first_column():\n",
    "    input_data = [\n",
    "        [0.6, 0.2, 0.2], \n",
    "        [0.1, 0.8, 0.1],\n",
    "        [0.2, 0.1, 0.7]\n",
    "    ]\n",
    "    \n",
    "    categories = [\"Adelie\", \"Gentoo\", \"Chinstrap\"]\n",
    "    \n",
    "    response = predict_fn(input_data, categories)\n",
    "    \n",
    "    assert response == [\n",
    "        (\"Adelie\", 0.6),\n",
    "        (\"Gentoo\", 0.8),\n",
    "        (\"Chinstrap\", 0.7)\n",
    "    ]\n",
    "\n",
    "\n",
    "def test_output_does_not_return_array_if_single_prediction():\n",
    "    prediction = [(\"Adelie\", 0.6)]\n",
    "    response, _ = output_fn(prediction, \"application/json\")\n",
    "\n",
    "    assert response[\"prediction\"] == \"Adelie\"\n",
    "\n",
    "\n",
    "def test_output_returns_array_if_multiple_predictions():\n",
    "    prediction = [(\"Adelie\", 0.6), (\"Gentoo\", 0.8)]\n",
    "    response, _ = output_fn(prediction, \"application/json\")\n",
    "\n",
    "    assert len(response) == 2\n",
    "    assert response[0][\"prediction\"] == \"Adelie\"\n",
    "    assert response[1][\"prediction\"] == \"Gentoo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_pipeline_model = Join(\n",
    "    on=\"/\",\n",
    "    values=[\n",
    "        preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "            \"model\"\n",
    "        ].S3Output.S3Uri,\n",
    "        \"model.tar.gz\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "preprocessing_model = SKLearnModel(\n",
    "    model_data=transformation_pipeline_model,\n",
    "    entry_point=\"preprocessing_component.py\",\n",
    "    source_dir=str(INFERENCE_CODE_FOLDER),\n",
    "    framework_version=\"1.2-1\",\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processing_model = SKLearnModel(\n",
    "    model_data=transformation_pipeline_model,\n",
    "    entry_point=\"postprocessing_component.py\",\n",
    "    source_dir=str(INFERENCE_CODE_FOLDER),\n",
    "    framework_version=\"1.2-1\",\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pipeline import PipelineModel\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    name=\"inference-model\",\n",
    "    models=[preprocessing_model, tensorflow_model, post_processing_model],\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_MODEL_PACKAGE_GROUP = \"pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlos/Projects/penguin-classifier/.venv/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "register_model_step = ModelStep(\n",
    "    name=\"register\",\n",
    "    display_name=\"register-model\",\n",
    "    step_args=pipeline_model.register(\n",
    "        model_package_group_name=PIPELINE_MODEL_PACKAGE_GROUP,\n",
    "        model_metrics=model_metrics,\n",
    "        approval_status=\"PendingManualApproval\",\n",
    "        content_types=[\"text/csv\", \"application/json\"],\n",
    "        response_types=[\"text/csv\", \"application/json\"],\n",
    "        inference_instances=[config[\"instance_type\"]],\n",
    "        transform_instances=[config[\"instance_type\"]],\n",
    "        domain=\"MACHINE_LEARNING\",\n",
    "        task=\"CLASSIFICATION\",\n",
    "        framework=\"TENSORFLOW\",\n",
    "        framework_version=config[\"framework_version\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_step = ConditionStep(\n",
    "    name=\"check-model-accuracy\",\n",
    "    conditions=[condition],\n",
    "    if_steps=[register_model_step] if not LOCAL_MODE else [],\n",
    "    else_steps=[fail_step],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded None to s3://sigmoidal-bucket/model-pipeline/code/968040ffa8955b343adf7f02b3f05a26/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sigmoidal-bucket/model-pipeline/code/2c207c809cb0e0e9a1d77e5247f961f9/runproc.sh\n",
      "/Users/carlos/Projects/penguin-classifier/.venv/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-2:035306718946:pipeline/model-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': 'c4106408-1a40-4151-924f-eded49ab567a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c4106408-1a40-4151-924f-eded49ab567a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '82',\n",
       "   'date': 'Fri, 01 Dec 2023 14:16:45 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model__pipeline = Pipeline(\n",
    "    name=\"model-pipeline\",\n",
    "    parameters=[dataset_location, accuracy_threshold],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "        tune_model_step if USE_TUNING_STEP else train_model_step,\n",
    "        evaluate_model_step,\n",
    "        condition_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")\n",
    "\n",
    "model__pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-2:035306718946:pipeline/model-pipeline/execution/c0z3ozt5uhqy', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x1032fcd50>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model__pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
