{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import ipytest\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from src.paths import CODE_FOLDER, DATA_DIR, INFERENCE_CODE_FOLDER, PARENT_DIR\n",
    "\n",
    "sys.path.append(str(PARENT_DIR / CODE_FOLDER))\n",
    "sys.path.append(str(PARENT_DIR / INFERENCE_CODE_FOLDER))\n",
    "\n",
    "DATA_FILE_PATH = DATA_DIR / \"penguins.csv\"\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/carlos/Projects/palmer-penguins-classification/notebooks',\n",
       " '/Users/carlos/.pyenv/versions/3.11.3/lib/python311.zip',\n",
       " '/Users/carlos/.pyenv/versions/3.11.3/lib/python3.11',\n",
       " '/Users/carlos/.pyenv/versions/3.11.3/lib/python3.11/lib-dynload',\n",
       " '',\n",
       " '/Users/carlos/Projects/palmer-penguins-classification/.venv/lib/python3.11/site-packages',\n",
       " '/Users/carlos/Projects/palmer-penguins-classification',\n",
       " '/Users/carlos/Projects/palmer-penguins-classification/src',\n",
       " '/Users/carlos/Projects/palmer-penguins-classification/src/inference',\n",
       " '/Users/carlos/Projects/palmer-penguins-classification/src',\n",
       " '/Users/carlos/Projects/palmer-penguins-classification/src/inference']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list sys.path to check if the path is added\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of 'drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /Users/carlos/Projects/palmer-penguins-classification/src/preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/preprocessor.py\n",
    "\"\"\"\n",
    "This module preprocesses data for machine learning tasks. It includes functions to read data from CSV files,\n",
    "split the data into training, validation, and test sets, save baseline data, transform data, and save the\n",
    "processed data and models.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Import statements...\n",
    "from typing import Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def preprocess(base_directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Preprocess the data by loading, splitting, transforming, saving the splits, and saving the model.\n",
    "\n",
    "    Args:\n",
    "        base_directory: The base directory where the input data and outputs will be managed.\n",
    "    \"\"\"\n",
    "    # 1. Load supplied data\n",
    "    df = _read_data_from_csv_files(base_directory)\n",
    "\n",
    "    # 2. Split data into train and test sets\n",
    "    df_train, df_validation, df_test = _split_data(df)\n",
    "\n",
    "    # 3. Save baseline data\n",
    "    _save_baselines(base_directory, df_train, df_test)\n",
    "\n",
    "    # 3. Transform the train and test sets\n",
    "    target_transformer = ColumnTransformer(\n",
    "        transformers=[(\"species\", OrdinalEncoder(), [0])]\n",
    "    )\n",
    "\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"mean\"), StandardScaler()\n",
    "    )\n",
    "\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"), OneHotEncoder()\n",
    "    )\n",
    "\n",
    "    features_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"numeric\",\n",
    "                numeric_transformer,\n",
    "                make_column_selector(dtype_exclude=\"object\"),\n",
    "            ),\n",
    "            (\"categorical\", categorical_transformer, [\"island\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    y_train = target_transformer.fit_transform(\n",
    "        np.array(df_train.species.values).reshape(-1, 1)\n",
    "    )\n",
    "    y_validation = target_transformer.transform(\n",
    "        np.array(df_validation.species.values).reshape(-1, 1)\n",
    "    )\n",
    "    y_test = target_transformer.transform(\n",
    "        np.array(df_test.species.values).reshape(-1, 1)\n",
    "    )\n",
    "\n",
    "    df_train = df_train.drop(columns=[\"species\"], axis=1)\n",
    "    df_validation = df_validation.drop(columns=[\"species\"], axis=1)\n",
    "    df_test = df_test.drop(columns=[\"species\"], axis=1)\n",
    "\n",
    "    X_train = features_transformer.fit_transform(df_train)\n",
    "    X_validation = features_transformer.transform(df_validation)\n",
    "    X_test = features_transformer.transform(df_test)\n",
    "\n",
    "    # 4. Save the train and test splits\n",
    "    _save_splits(\n",
    "        base_directory, X_train, y_train, X_validation, y_validation, X_test, y_test\n",
    "    )\n",
    "\n",
    "    # 5. Save the model (transformers) in tar.gz format\n",
    "    _save_model(base_directory, target_transformer, features_transformer)\n",
    "\n",
    "\n",
    "def _read_data_from_csv_files(base_directory: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read and concatenate data from CSV files located in the input directory.\n",
    "\n",
    "    Args:\n",
    "        base_directory: The directory where CSV files are located.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the concatenated data.\n",
    "    \"\"\"\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = [file for file in input_directory.glob(\"*.csv\")]\n",
    "\n",
    "    if len(files) == 0:\n",
    "        raise ValueError(f\"No csv files found in {input_directory}\")\n",
    "\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "    df = pd.concat(raw_data)\n",
    "\n",
    "    # Shuffle the data\n",
    "    return df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "def _split_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split the DataFrame into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to be split.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the training, validation, and test DataFrames.\n",
    "    \"\"\"\n",
    "    df_train, temp = train_test_split(df, test_size=0.3)\n",
    "    df_validation, df_test = train_test_split(temp, test_size=0.5)\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "\n",
    "def _save_baselines(\n",
    "    base_directory: str, df_train: pd.DataFrame = None, df_test: pd.DataFrame = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save baseline versions of the training and test data sets.\n",
    "\n",
    "    Args:\n",
    "        base_directory: Directory where the baseline data will be saved.\n",
    "        df_train: Training data DataFrame.\n",
    "        df_test: Test data DataFrame.\n",
    "    \"\"\"\n",
    "    for split, data in [(\"train\", df_train), (\"test\", df_test)]:\n",
    "        baseline_path = Path(base_directory) / f\"{split}-baseline\"\n",
    "        baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df = data.copy().dropna()\n",
    "\n",
    "        # Save header only for the train baseline\n",
    "        header = True if split == \"train\" else False\n",
    "        df.to_csv(baseline_path / f\"{split}-baseline.csv\", index=False, header=header)\n",
    "\n",
    "\n",
    "def _save_splits(\n",
    "    base_directory: str,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_validation: np.ndarray,\n",
    "    y_validation: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save the training, validation, and test sets after concatenating features with their respective targets.\n",
    "\n",
    "    Args:\n",
    "        base_directory: Directory where the data splits will be saved.\n",
    "        X_train: Features of the training set.\n",
    "        y_train: Target of the training set.\n",
    "        X_validation: Features of the validation set.\n",
    "        y_validation: Target of the validation set.\n",
    "        X_test: Features of the test set.\n",
    "        y_test: Target of the test set.\n",
    "    \"\"\"\n",
    "    train = np.concatenate((X_train, y_train), axis=1)\n",
    "    validation = np.concatenate((X_validation, y_validation), axis=1)\n",
    "    test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    validation_path = Path(base_directory) / \"validation\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", index=False, header=False)\n",
    "    pd.DataFrame(validation).to_csv(\n",
    "        validation_path / \"validation.csv\", index=False, header=False\n",
    "    )\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", index=False, header=False)\n",
    "\n",
    "\n",
    "def _save_model(base_directory: str, target_transformer, features_transformers) -> None:\n",
    "    \"\"\"\n",
    "    Save the preprocessing model (transformers) in tar.gz format.\n",
    "\n",
    "    Args:\n",
    "        base_directory: Directory where the model will be saved.\n",
    "        target_transformer: The transformer used for the target variable.\n",
    "        features_transformers: The transformers used for the feature variables.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as directory:\n",
    "        joblib.dump(target_transformer, os.path.join(directory, \"target.joblib\"))\n",
    "        joblib.dump(features_transformers, os.path.join(directory, \"features.joblib\"))\n",
    "\n",
    "        model_path = Path(base_directory) / \"model\"\n",
    "        model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with tarfile.open(f\"{str(model_path / 'model.tar.gz')}\", \"w:gz\") as tar:\n",
    "            tar.add(os.path.join(directory, \"target.joblib\"), arcname=\"target.joblib\")\n",
    "            tar.add(\n",
    "                os.path.join(directory, \"features.joblib\"), arcname=\"features.joblib\"\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(base_directory=\"/opt/ml/processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "\n",
    "import os\n",
    "import pytest\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import tarfile\n",
    "from preprocessor import preprocess\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"function\", autouse=False)\n",
    "def directory():\n",
    "    directory = tempfile.mkdtemp()\n",
    "    \n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    shutil.copy2(DATA_FILE_PATH, input_directory / \"data.csv\")\n",
    "\n",
    "    directory = Path(directory)\n",
    "    preprocess(directory)\n",
    "\n",
    "    yield directory\n",
    "    \n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "def test_preprocess_generate_baselines(directory):\n",
    "    output_directories = os.listdir(directory)\n",
    "\n",
    "    assert \"train-baseline\" in output_directories\n",
    "    assert \"test-baseline\" in output_directories\n",
    "\n",
    "\n",
    "def test_preprocess_generate_data_splits(directory):\n",
    "    output_directories = os.listdir(directory)\n",
    "\n",
    "    assert \"train\" in output_directories\n",
    "    assert \"validation\" in output_directories\n",
    "    assert \"test\" in output_directories\n",
    "\n",
    "\n",
    "def test_preprocess_creates_two_models(directory):\n",
    "    model_path = directory / \"model\"\n",
    "\n",
    "    tar = tarfile.open(model_path / \"model.tar.gz\", \"r:gz\")\n",
    "\n",
    "    assert \"target.joblib\" in tar.getnames()\n",
    "    assert \"features.joblib\" in tar.getnames()\n",
    "\n",
    "\n",
    "def tests_splits_are_transformed(directory):\n",
    "    train = pd.read_csv(directory / \"train\" / \"train.csv\", header=None)\n",
    "    validation = pd.read_csv(directory / \"validation\" / \"validation.csv\", header=None)\n",
    "    test = pd.read_csv(directory / \"test\" / \"test.csv\", header=None)\n",
    "\n",
    "    # The number of features should be 7\n",
    "    # * 3 - island (one-hot encoded)\n",
    "    # * 1 - culmen_length_mm\n",
    "    # * 1 - culmen_depth_mm\n",
    "    # * 1 - flipper_length_mm\n",
    "    # * 1 - body_mass_g\n",
    "    num_features = 7\n",
    "\n",
    "    # The number of targets should be 1\n",
    "    assert train.shape[1] == num_features + 1\n",
    "    assert validation.shape[1] == num_features + 1\n",
    "    assert test.shape[1] == num_features + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
