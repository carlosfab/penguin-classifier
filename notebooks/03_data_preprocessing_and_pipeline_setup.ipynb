{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "# Load necessary extensions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "\n",
    "# Standard library imports\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party library imports\n",
    "import ipytest\n",
    "import json\n",
    "\n",
    "# Local imports\n",
    "from src.paths import CODE_FOLDER, DATA_DIR, INFERENCE_CODE_FOLDER, PARENT_DIR\n",
    "\n",
    "sys.path.append(str(PARENT_DIR / CODE_FOLDER))\n",
    "sys.path.append(str(PARENT_DIR / INFERENCE_CODE_FOLDER))\n",
    "\n",
    "DATA_FILE_PATH = DATA_DIR / \"penguins.csv\"\n",
    "\n",
    "ipytest.autoconfig(raise_on_error=True)\n",
    "\n",
    "# Prevent SageMaker SDK to log events related to the default\n",
    "# configuration using the INFO level\n",
    "logging.getLogger(\"sagemaker.config\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket = os.getenv(\"BUCKET\")\n",
    "role = os.getenv(\"ROLE\")\n",
    "\n",
    "S3_LOCATION = f\"s3://{bucket}/penguins\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession, LocalPipelineSession\n",
    "\n",
    "pipeline_session = PipelineSession(default_bucket=bucket) if not LOCAL_MODE else None\n",
    "\n",
    "if LOCAL_MODE:\n",
    "    config = {\n",
    "        \"session\": LocalPipelineSession(default_bucket=bucket),\n",
    "        \"instance_type\": \"local\",\n",
    "        \"image\": None\n",
    "    }\n",
    "else:\n",
    "    config = {\n",
    "        \"session\": pipeline_session,\n",
    "        \"instance_type\": \"ml.m5.xlarge\",\n",
    "        \"image\": None\n",
    "    }\n",
    "\n",
    "config[\"framework_version\"] = \"2.11\"\n",
    "config[\"py_version\"] = \"py39\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "iam_client = boto3.client(\"iam\")\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of 'drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Preprocessing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /Users/carlos/Projects/penguin-classifier/src/preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/preprocessor.py\n",
    "\"\"\"\n",
    "This module preprocesses data for machine learning tasks. It includes functions to read data from CSV files,\n",
    "split the data into training, validation, and test sets, save baseline data, transform data, and save the\n",
    "processed data and models.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Import statements...\n",
    "from typing import Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def preprocess(base_directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Preprocess the data by loading, splitting, transforming, saving the splits, and saving the model.\n",
    "\n",
    "    Args:\n",
    "        base_directory: The base directory where the input data and outputs will be managed.\n",
    "    \"\"\"\n",
    "    # 1. Load supplied data\n",
    "    df = _read_data_from_csv_files(base_directory)\n",
    "\n",
    "    # 2. Split data into train and test sets\n",
    "    df_train, df_validation, df_test = _split_data(df)\n",
    "\n",
    "    # 3. Save baseline data\n",
    "    _save_baselines(base_directory, df_train, df_test)\n",
    "\n",
    "    # 3. Transform the train and test sets\n",
    "    target_transformer = ColumnTransformer(\n",
    "        transformers=[(\"species\", OrdinalEncoder(), [0])]\n",
    "    )\n",
    "\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"mean\"), StandardScaler()\n",
    "    )\n",
    "\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"), OneHotEncoder()\n",
    "    )\n",
    "\n",
    "    features_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"numeric\",\n",
    "                numeric_transformer,\n",
    "                make_column_selector(dtype_exclude=\"object\"),\n",
    "            ),\n",
    "            (\"categorical\", categorical_transformer, [\"island\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    y_train = target_transformer.fit_transform(\n",
    "        np.array(df_train.species.values).reshape(-1, 1)\n",
    "    )\n",
    "    y_validation = target_transformer.transform(\n",
    "        np.array(df_validation.species.values).reshape(-1, 1)\n",
    "    )\n",
    "    y_test = target_transformer.transform(\n",
    "        np.array(df_test.species.values).reshape(-1, 1)\n",
    "    )\n",
    "\n",
    "    df_train = df_train.drop(columns=[\"species\"], axis=1)\n",
    "    df_validation = df_validation.drop(columns=[\"species\"], axis=1)\n",
    "    df_test = df_test.drop(columns=[\"species\"], axis=1)\n",
    "\n",
    "    X_train = features_transformer.fit_transform(df_train)\n",
    "    X_validation = features_transformer.transform(df_validation)\n",
    "    X_test = features_transformer.transform(df_test)\n",
    "\n",
    "    # 4. Save the train and test splits\n",
    "    _save_splits(\n",
    "        base_directory, X_train, y_train, X_validation, y_validation, X_test, y_test\n",
    "    )\n",
    "\n",
    "    # 5. Save the model (transformers) in tar.gz format\n",
    "    _save_model(base_directory, target_transformer, features_transformer)\n",
    "\n",
    "\n",
    "def _read_data_from_csv_files(base_directory: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read and concatenate data from CSV files located in the input directory.\n",
    "\n",
    "    Args:\n",
    "        base_directory: The directory where CSV files are located.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the concatenated data.\n",
    "    \"\"\"\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = [file for file in input_directory.glob(\"*.csv\")]\n",
    "\n",
    "    if len(files) == 0:\n",
    "        raise ValueError(f\"No csv files found in {input_directory}\")\n",
    "\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "    df = pd.concat(raw_data)\n",
    "\n",
    "    # Shuffle the data\n",
    "    return df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "def _split_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split the DataFrame into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to be split.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the training, validation, and test DataFrames.\n",
    "    \"\"\"\n",
    "    df_train, temp = train_test_split(df, test_size=0.3)\n",
    "    df_validation, df_test = train_test_split(temp, test_size=0.5)\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "\n",
    "def _save_baselines(\n",
    "    base_directory: str, df_train: pd.DataFrame = None, df_test: pd.DataFrame = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save baseline versions of the training and test data sets.\n",
    "\n",
    "    Args:\n",
    "        base_directory: Directory where the baseline data will be saved.\n",
    "        df_train: Training data DataFrame.\n",
    "        df_test: Test data DataFrame.\n",
    "    \"\"\"\n",
    "    for split, data in [(\"train\", df_train), (\"test\", df_test)]:\n",
    "        baseline_path = Path(base_directory) / f\"{split}-baseline\"\n",
    "        baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df = data.copy().dropna()\n",
    "\n",
    "        # Save header only for the train baseline\n",
    "        header = True if split == \"train\" else False\n",
    "        df.to_csv(baseline_path / f\"{split}-baseline.csv\", index=False, header=header)\n",
    "\n",
    "\n",
    "def _save_splits(\n",
    "    base_directory: str,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_validation: np.ndarray,\n",
    "    y_validation: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save the training, validation, and test sets after concatenating features with their respective targets.\n",
    "\n",
    "    Args:\n",
    "        base_directory: Directory where the data splits will be saved.\n",
    "        X_train: Features of the training set.\n",
    "        y_train: Target of the training set.\n",
    "        X_validation: Features of the validation set.\n",
    "        y_validation: Target of the validation set.\n",
    "        X_test: Features of the test set.\n",
    "        y_test: Target of the test set.\n",
    "    \"\"\"\n",
    "    train = np.concatenate((X_train, y_train), axis=1)\n",
    "    validation = np.concatenate((X_validation, y_validation), axis=1)\n",
    "    test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    validation_path = Path(base_directory) / \"validation\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", index=False, header=False)\n",
    "    pd.DataFrame(validation).to_csv(\n",
    "        validation_path / \"validation.csv\", index=False, header=False\n",
    "    )\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", index=False, header=False)\n",
    "\n",
    "\n",
    "def _save_model(base_directory: str, target_transformer, features_transformers) -> None:\n",
    "    \"\"\"\n",
    "    Save the preprocessing model (transformers) in tar.gz format.\n",
    "\n",
    "    Args:\n",
    "        base_directory: Directory where the model will be saved.\n",
    "        target_transformer: The transformer used for the target variable.\n",
    "        features_transformers: The transformers used for the feature variables.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as directory:\n",
    "        joblib.dump(target_transformer, os.path.join(directory, \"target.joblib\"))\n",
    "        joblib.dump(features_transformers, os.path.join(directory, \"features.joblib\"))\n",
    "\n",
    "        model_path = Path(base_directory) / \"model\"\n",
    "        model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with tarfile.open(f\"{str(model_path / 'model.tar.gz')}\", \"w:gz\") as tar:\n",
    "            tar.add(os.path.join(directory, \"target.joblib\"), arcname=\"target.joblib\")\n",
    "            tar.add(\n",
    "                os.path.join(directory, \"features.joblib\"), arcname=\"features.joblib\"\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(base_directory=\"/opt/ml/processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.17s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import tarfile\n",
    "from preprocessor import preprocess\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"function\", autouse=False)\n",
    "def directory():\n",
    "    directory = tempfile.mkdtemp()\n",
    "    \n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    shutil.copy2(DATA_FILE_PATH, input_directory / \"data.csv\")\n",
    "\n",
    "    directory = Path(directory)\n",
    "    preprocess(directory)\n",
    "\n",
    "    yield directory\n",
    "    \n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "def test_preprocess_generate_baselines(directory):\n",
    "    output_directories = os.listdir(directory)\n",
    "\n",
    "    assert \"train-baseline\" in output_directories\n",
    "    assert \"test-baseline\" in output_directories\n",
    "\n",
    "\n",
    "def test_preprocess_generate_data_splits(directory):\n",
    "    output_directories = os.listdir(directory)\n",
    "\n",
    "    assert \"train\" in output_directories\n",
    "    assert \"validation\" in output_directories\n",
    "    assert \"test\" in output_directories\n",
    "\n",
    "\n",
    "def test_preprocess_creates_two_models(directory):\n",
    "    model_path = directory / \"model\"\n",
    "\n",
    "    tar = tarfile.open(model_path / \"model.tar.gz\", \"r:gz\")\n",
    "\n",
    "    assert \"target.joblib\" in tar.getnames()\n",
    "    assert \"features.joblib\" in tar.getnames()\n",
    "\n",
    "\n",
    "def tests_splits_are_transformed(directory):\n",
    "    train = pd.read_csv(directory / \"train\" / \"train.csv\", header=None)\n",
    "    validation = pd.read_csv(directory / \"validation\" / \"validation.csv\", header=None)\n",
    "    test = pd.read_csv(directory / \"test\" / \"test.csv\", header=None)\n",
    "\n",
    "    # The number of features should be 7\n",
    "    # * 3 - island (one-hot encoded)\n",
    "    # * 1 - culmen_length_mm\n",
    "    # * 1 - culmen_depth_mm\n",
    "    # * 1 - flipper_length_mm\n",
    "    # * 1 - body_mass_g\n",
    "    num_features = 7\n",
    "\n",
    "    # The number of targets should be 1\n",
    "    assert train.shape[1] == num_features + 1\n",
    "    assert validation.shape[1] == num_features + 1\n",
    "    assert test.shape[1] == num_features + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Processing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"P15D\") # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString\n",
    "\n",
    "dataset_location = ParameterString(\n",
    "    name=\"dataset_location\",\n",
    "    default_value=f\"{S3_LOCATION}/data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    framework_version=\"1.2-1\",\n",
    "    base_job_name=\"preprocess-data\",\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlos/Projects/penguin-classifier/.venv/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"preprocess-data\",\n",
    "    step_args=processor.run(\n",
    "        code=f\"{CODE_FOLDER}/preprocessor.py\",\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=dataset_location,\n",
    "                destination=\"/opt/ml/processing/input\"\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train\",\n",
    "                source=\"/opt/ml/processing/train\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"validation\",\n",
    "                source=\"/opt/ml/processing/validation\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/validation\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test\",\n",
    "                source=\"/opt/ml/processing/test\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"model\",\n",
    "                source=\"/opt/ml/processing/model\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/model\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train-baseline\",\n",
    "                source=\"/opt/ml/processing/train-baseline\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train-baseline\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test-baseline\",\n",
    "                source=\"/opt/ml/processing/test-baseline\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test-baseline\",\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'preprocessing-pipeline'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "\n",
    "pipeline_definition_config = PipelineDefinitionConfig(\n",
    "    use_custom_job_prefix=True\n",
    ")\n",
    "\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    name=\"preprocessing-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[preprocessing_step],\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    ")\n",
    "\n",
    "preprocessing_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting execution for pipeline preprocessing-pipeline. Execution ID is b79c70ff-11bb-4273-b1d8-316854a38b0f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline step: 'preprocess-data'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.local.image:'Docker Compose' found using Docker CLI.\n",
      "INFO:sagemaker.local.local_session:Starting processing job\n",
      "INFO:sagemaker.local.image:Using the long-lived AWS credentials found in session\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-7gha9:\n",
      "    container_name: ej2dydfzed-algo-1-7gha9\n",
      "    entrypoint:\n",
      "    - python3\n",
      "    - /opt/ml/processing/input/code/preprocessor.py\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: 257758044811.dkr.ecr.us-east-2.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-7gha9\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpbqijv945/algo-1-7gha9/output:/opt/ml/output\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpbqijv945/algo-1-7gha9/config:/opt/ml/config\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpi0_fkrjq:/opt/ml/processing/input\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmp0usjwz12:/opt/ml/processing/input/code\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpggi30xk0/output/train:/opt/ml/processing/train\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpggi30xk0/output/validation:/opt/ml/processing/validation\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpggi30xk0/output/test:/opt/ml/processing/test\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpggi30xk0/output/model:/opt/ml/processing/model\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpggi30xk0/output/train-baseline:/opt/ml/processing/train-baseline\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpggi30xk0/output/test-baseline:/opt/ml/processing/test-baseline\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpbqijv945/shared:/opt/ml/shared\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker compose -f /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpbqijv945/docker-compose.yaml up --build --abort-on-container-exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2023-11-29T09:30:56-03:00\" level=warning msg=\"a network with name sagemaker-local exists but was not created for project \\\"tmpbqijv945\\\".\\nSet `external: true` to use an existing network\"\n",
      " Container ej2dydfzed-algo-1-7gha9  Creating\n",
      " Container ej2dydfzed-algo-1-7gha9  Created\n",
      "Attaching to ej2dydfzed-algo-1-7gha9\n",
      "ej2dydfzed-algo-1-7gha9 exited with code 0\n",
      "Aborting on container exit...\n",
      " Container ej2dydfzed-algo-1-7gha9  Stopping\n",
      " Container ej2dydfzed-algo-1-7gha9  Stopped\n",
      "===== Job Complete =====\n",
      "Pipeline step 'preprocess-data' SUCCEEDED.\n",
      "Pipeline execution b79c70ff-11bb-4273-b1d8-316854a38b0f SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.local.entities._LocalPipelineExecution at 0x148ae2210>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "preprocessing_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models and the Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/carlos/Projects/penguin-classifier/src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "def train(model_directory: str, train_path: str, validation_path: str, epochs: int =50, batch_size: int=32) -> None:\n",
    "    \"\"\"\n",
    "    Train a model using the training and validation data sets.\n",
    "\n",
    "    Args:\n",
    "        model_directory: Directory where the model will be saved.\n",
    "        train_path: Path to the training data set.\n",
    "        validation_path: Path to the validation data set.\n",
    "        epochs: Number of epochs to train the model.\n",
    "        batch_size: Batch size used during training.\n",
    "    \"\"\"\n",
    "    # Load training and validation data sets\n",
    "    X_train = pd.read_csv(Path(train_path) / \"train.csv\")\n",
    "    y_train = X_train[X_train.columns[-1]]\n",
    "    X_train.drop(X_train.columns[-1], axis=1, inplace=True)\n",
    "\n",
    "    X_validation = pd.read_csv(Path(validation_path) / \"validation.csv\")\n",
    "    y_validation = X_validation[X_validation.columns[-1]]\n",
    "    X_validation.drop(X_validation.columns[-1], axis=1, inplace=True)\n",
    "\n",
    "    # Build a Sequential model\n",
    "    model = Sequential([\n",
    "        Dense(10, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "        Dense(8, activation=\"relu\"),\n",
    "        Dense(3, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=SGD(learning_rate=0.01),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_validation, y_validation),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_validation)\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    print(f\"Validation accuracy: {accuracy_score(y_validation, predictions)}\")\n",
    "\n",
    "    # Save the model\n",
    "    model_filepath = Path(model_directory) / \"001\"\n",
    "    model.save(model_filepath)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    train(\n",
    "        model_directory=os.environ[\"SM_MODEL_DIR\"],\n",
    "        train_path=os.environ[\"SM_CHANNEL_TRAIN\"],\n",
    "        validation_path=os.environ[\"SM_CHANNEL_VALIDATION\"],\n",
    "\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 1s - loss: 1.1733 - accuracy: 0.3849 - val_loss: 1.1068 - val_accuracy: 0.4314 - 894ms/epoch - 112ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Validation accuracy: 0.43137254901960786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmp3lvhjb76/model/001/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 1.88s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import pytest\n",
    "import tempfile\n",
    "import joblib\n",
    "\n",
    "from preprocessor import preprocess\n",
    "from train import train\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"function\", autouse=False)\n",
    "def directory():\n",
    "    directory = tempfile.mkdtemp()\n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(DATA_FILE_PATH, input_directory / \"data.csv\")\n",
    "    \n",
    "    directory = Path(directory)\n",
    "    \n",
    "    preprocess(base_directory=directory)\n",
    "    train(\n",
    "        model_directory=directory / \"model\",\n",
    "        train_path=directory / \"train\", \n",
    "        validation_path=directory / \"validation\",\n",
    "        epochs=1\n",
    "    )\n",
    "    \n",
    "    yield directory\n",
    "    \n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "def test_train_saves_a_folder_with_model_assets(directory):\n",
    "    output = os.listdir(directory / \"model\")\n",
    "    assert \"001\" in output\n",
    "    \n",
    "    assets = os.listdir(directory / \"model\" / \"001\")\n",
    "    assert \"saved_model.pb\" in assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    base_job_name=\"training\",\n",
    "    entry_point=f\"{CODE_FOLDER}/train.py\",\n",
    "    hyperparameters={\n",
    "        \"epochs\": 50,\n",
    "        \"batch_size\": 32\n",
    "    },\n",
    "    metrics_definitions=[\n",
    "        {\"Name\": \"loss\", \"Regex\": \"loss: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"accuracy\", \"Regex\": \"accuracy: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"val_loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}\n",
    "    ],\n",
    "    image_uri=config[\"image\"],\n",
    "    framework_version=config[\"framework_version\"],\n",
    "    py_version=config[\"py_version\"],\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    disable_profiler=True,\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlos/Projects/penguin-classifier/.venv/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_model_step = TrainingStep(\n",
    "    name=\"train-model\",\n",
    "    step_args=estimator.fit(\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"validation\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "        }\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TUNING_STEP = False and not LOCAL_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.parameter import IntegerParameter\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name=\"val_accuracy\",\n",
    "    objective_type=\"Maximize\",\n",
    "    hyperparameter_ranges={\n",
    "        \"epochs\": IntegerParameter(10, 50),\n",
    "    },\n",
    "    metric_definitions=[{\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}],\n",
    "    max_jobs=3,\n",
    "    max_parallel_jobs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import TuningStep\n",
    "\n",
    "tune_model_step = TuningStep(\n",
    "    name=\"tune-model\",\n",
    "    step_args=tuner.fit(\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"validation\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "        },\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'training-pipeline'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pipeline = Pipeline(\n",
    "    name=\"training-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "        tune_model_step if USE_TUNING_STEP else train_model_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")\n",
    "\n",
    "training_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting execution for pipeline training-pipeline. Execution ID is 74ff83f6-3a19-494c-873c-75b6285d5977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline step: 'preprocess-data'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.local.image:'Docker Compose' found using Docker CLI.\n",
      "INFO:sagemaker.local.local_session:Starting processing job\n",
      "INFO:sagemaker.local.image:Using the long-lived AWS credentials found in session\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-sm9z4:\n",
      "    container_name: 7sl65b6h3l-algo-1-sm9z4\n",
      "    entrypoint:\n",
      "    - python3\n",
      "    - /opt/ml/processing/input/code/preprocessor.py\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: 257758044811.dkr.ecr.us-east-2.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-sm9z4\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpek9y26g5/algo-1-sm9z4/output:/opt/ml/output\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpek9y26g5/algo-1-sm9z4/config:/opt/ml/config\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpq_kf6xv0:/opt/ml/processing/input\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmp6rf2rzxt:/opt/ml/processing/input/code\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmp2xvu1v77/output/train:/opt/ml/processing/train\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmp2xvu1v77/output/validation:/opt/ml/processing/validation\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmp2xvu1v77/output/test:/opt/ml/processing/test\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmp2xvu1v77/output/model:/opt/ml/processing/model\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmp2xvu1v77/output/train-baseline:/opt/ml/processing/train-baseline\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmp2xvu1v77/output/test-baseline:/opt/ml/processing/test-baseline\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpek9y26g5/shared:/opt/ml/shared\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker compose -f /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpek9y26g5/docker-compose.yaml up --build --abort-on-container-exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2023-11-29T10:02:23-03:00\" level=warning msg=\"a network with name sagemaker-local exists but was not created for project \\\"tmpek9y26g5\\\".\\nSet `external: true` to use an existing network\"\n",
      " Container 7sl65b6h3l-algo-1-sm9z4  Creating\n",
      " Container 7sl65b6h3l-algo-1-sm9z4  Created\n",
      "Attaching to 7sl65b6h3l-algo-1-sm9z4\n",
      "7sl65b6h3l-algo-1-sm9z4 exited with code 0\n",
      "Aborting on container exit...\n",
      " Container 7sl65b6h3l-algo-1-sm9z4  Stopping\n",
      " Container 7sl65b6h3l-algo-1-sm9z4  Stopped\n",
      "===== Job Complete =====\n",
      "Pipeline step 'preprocess-data' SUCCEEDED.\n",
      "Starting pipeline step: 'train-model'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.local.image:'Docker Compose' found using Docker CLI.\n",
      "INFO:sagemaker.local.local_session:Starting training job\n",
      "INFO:sagemaker.local.image:Using the long-lived AWS credentials found in session\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-6qyoe:\n",
      "    command: train\n",
      "    container_name: k7nk09vkpw-algo-1-6qyoe\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: 763104351884.dkr.ecr.us-east-2.amazonaws.com/tensorflow-training:2.11-cpu-py39\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-6qyoe\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpsvwwmy6a/algo-1-6qyoe/output:/opt/ml/output\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpsvwwmy6a/algo-1-6qyoe/output/data:/opt/ml/output/data\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpsvwwmy6a/algo-1-6qyoe/input:/opt/ml/input\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpsvwwmy6a/model:/opt/ml/model\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmp03c3drkj:/opt/ml/input/data/train\n",
      "    - /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpk_ri2q6o:/opt/ml/input/data/validation\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker compose -f /private/var/folders/ft/p0zj8zms6wldqq4t_fyn6w300000gn/T/tmpsvwwmy6a/docker-compose.yaml up --build --abort-on-container-exit\n",
      "INFO:sagemaker.local.image:docker command: docker pull 763104351884.dkr.ecr.us-east-2.amazonaws.com/tensorflow-training:2.11-cpu-py39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "training_pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
